{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from torch.utils.data import Dataset\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import json\n",
    "import abc\n",
    "from sklearn import svm\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClass(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, data_dir):\n",
    "        self.transcript_dir = os.path.join(data_dir, \"CCM_Transcript_Text_Data\")\n",
    "        self.file_list = sorted([str(filePath) for filePath in Path(self.transcript_dir).glob(\"**/*\") if filePath.is_file()])\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "baseInfo = BaseClass(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return open(self.data[idx], \"r\", encoding=\"utf-8\").read()\n",
    "    \n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return open(self.data[idx], \"r\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "json_file_path = \"text_labels_dict.json\"\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    labels = json.load(json_file)\n",
    "\n",
    "x_files = []\n",
    "y = []\n",
    "for file in baseInfo.file_list:\n",
    "    x_file_name = file.replace(\"data\\\\CCM_Transcript_Text_Data\\\\\", \"\")\n",
    "    if x_file_name in labels:\n",
    "        x_files.append(file)\n",
    "        y.append(labels[x_file_name])\n",
    "\n",
    "data_loader = DataLoader(CustomTrainDataset(x_files), batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings = []\n",
    "model.eval()  # Put model in evaluation mode\n",
    "with torch.no_grad():\n",
    "    for data in data_loader:\n",
    "        # Tokenize sentences and create attention masks\n",
    "        inputs = tokenizer(data, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # Get raw embeddings from the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Apply mean pooling to get sentence embeddings\n",
    "        embeddings = mean_pooling(outputs, attention_mask)\n",
    "        x_embeddings.append(embeddings)\n",
    "\n",
    "x_embeddings = torch.cat(x_embeddings, axis=0)\n",
    "\n",
    "X_np = x_embeddings.numpy()\n",
    "y_np = y.numpy()\n",
    "np.savez(f'X-y-whole-text-embeddings.npz', X=X_np, y=y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_length = 0\n",
    "for textfile in x_files:\n",
    "    with open(textfile, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        max_line_length = max(max_line_length, len(lines))\n",
    "print(\"max line length: \", max_line_length)\n",
    "\n",
    "X = torch.zeros(len(y), max_line_length, 768)\n",
    "model.eval()\n",
    "currIndex = 0\n",
    "with torch.no_grad():\n",
    "    for data in x_files:\n",
    "        sentence_split_data = [line.strip() for line in open(data, 'r')]\n",
    "        print(sentence_split_data)\n",
    "        inputs = tokenizer(sentence_split_data, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # Get raw embeddings from the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Apply mean pooling to get sentence embeddings\n",
    "        embeddings = mean_pooling(outputs, attention_mask)\n",
    "    \n",
    "        print(\"number of lines in e: \", embeddings.shape[0])\n",
    "\n",
    "        num_vectors_to_pad = max_line_length - embeddings.shape[0]\n",
    "        zero_padding = torch.zeros(num_vectors_to_pad, embeddings.shape[1])\n",
    "        padded_tensor = torch.cat((embeddings, zero_padding), dim=0)\n",
    "        X[currIndex] = padded_tensor\n",
    "        currIndex += 1\n",
    "\n",
    "X = X.numpy()\n",
    "np.savez(f'X-y-sequential-text-embeddings.npz', X=X_np, y=y_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
