{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import ShortTermFeatures, MidTermFeatures\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract medium term features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files_dir = 'data/Audio_Data'\n",
    "feature_vectors_dir = 'medium_term_audio_vectors'\n",
    "\n",
    "audio_labels_path = 'audio_labels_dict.json'\n",
    "with open(audio_labels_path, 'r') as file:\n",
    "    audio_labels_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size of 100 ms\n",
    "win = 0.1\n",
    "# Step size of 100 ms for sub-windows\n",
    "step = 0.1\n",
    "\n",
    "# For mid-term feature extraction\n",
    "mid_window = 1.0\n",
    "mid_step = 1.0\n",
    "short_window = 0.050\n",
    "short_step = 0.050\n",
    "\n",
    "def convert_to_wav(m4a_file_path, wav_file_path):\n",
    "    sound = AudioSegment.from_file(m4a_file_path, format='m4a')\n",
    "    file_handle = sound.export(wav_file_path, format='wav')\n",
    "    return file_handle\n",
    "\n",
    "def extract_features(audio_file_path):\n",
    "    # Read the audio file\n",
    "    [Fs, X] = audioBasicIO.read_audio_file(audio_file_path)\n",
    "    X = audioBasicIO.stereo_to_mono(X)\n",
    "    F, short_features, mid_feature_names = MidTermFeatures.mid_feature_extraction(X, Fs, mid_window*Fs, mid_step*Fs, short_window*Fs, short_step*Fs)\n",
    "    F = np.transpose(F)\n",
    "    print(f'shape: {F.shape}')\n",
    "    return F\n",
    "\n",
    "for filename in os.listdir(audio_files_dir):\n",
    "    if filename.lower().endswith('.m4a') and filename in audio_labels_dict:\n",
    "        m4a_file_path = os.path.join(audio_files_dir, filename)\n",
    "        wav_file_path = m4a_file_path.replace('.m4a', '.wav')\n",
    "\n",
    "        convert_to_wav(m4a_file_path, wav_file_path)\n",
    "\n",
    "        features = extract_features(wav_file_path)\n",
    "        \n",
    "        # Save the feature vectors to a file\n",
    "        feature_vector_path = os.path.join(feature_vectors_dir, filename[:-4] + '.npy')\n",
    "\n",
    "        np.save(feature_vector_path, features)\n",
    "        print(f'Processed {filename}')\n",
    "\n",
    "    elif filename.lower().endswith('.mp3') and filename in audio_labels_dict:\n",
    "        mp3_file_path = os.path.join(audio_files_dir, filename)\n",
    "        features = extract_features(mp3_file_path)\n",
    "        feature_vector_path = os.path.join(feature_vectors_dir, filename[:-4] + '.npy')\n",
    "        np.save(feature_vector_path, features)\n",
    "        print(f'Processed {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "  fnpm3 = f'{filename[:-4]}{str(\".mp3\")}'\n",
    "  fnm4a = f'{filename[:-4]}{str(\".m4a\")}'\n",
    "  if fnpm3 in audio_labels_dict:\n",
    "    y.append(audio_labels_dict[fnpm3])\n",
    "  elif fnm4a in audio_labels_dict:\n",
    "    y.append(audio_labels_dict[fnm4a])\n",
    "\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "    count += 1\n",
    "\n",
    "assert(len(y) == count)\n",
    "\n",
    "max_rows = 0\n",
    "# Iterate through all .npy files in the directory\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(feature_vectors_dir, filename)\n",
    "        \n",
    "        # Load the .npy file\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        # Update the maximum number of rows if this file has more rows\n",
    "        if data.shape[0] > max_rows:\n",
    "            max_rows = data.shape[0]\n",
    "\n",
    "embedding_size = data.shape[1]\n",
    "\n",
    "print(f\"The maximum value of n (number of rows) among all .npy files is: {max_rows}\")\n",
    "\n",
    "X = torch.zeros(len(y), max_rows, embedding_size)\n",
    "currIndex = 0\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "  file_path = os.path.join(feature_vectors_dir, filename) \n",
    "  e = torch.from_numpy(np.load(file_path))\n",
    "\n",
    "  fnpm3 = f'{filename[:-4]}{str(\".mp3\")}'\n",
    "  fnm4a = f'{filename[:-4]}{str(\".m4a\")}'\n",
    "\n",
    "  if fnpm3 in audio_labels_dict or fnm4a in audio_labels_dict:\n",
    "    num_vectors_to_pad = max_rows - e.shape[0]\n",
    "    zero_padding = torch.zeros(num_vectors_to_pad, e.shape[1])\n",
    "    padded_tensor = torch.cat((e, zero_padding), dim=0)\n",
    "    X[currIndex] = padded_tensor\n",
    "    currIndex += 1\n",
    "\n",
    "X_np = X.numpy()\n",
    "y_np = y.numpy()\n",
    "np.savez(f'X-y-medium-term-audio.npz', X=X_np, y=y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Short Term Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors_dir = 'short_term_audio_vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for short term feature extraction\n",
    "short_window = 0.050\n",
    "short_step = 0.025\n",
    "\n",
    "def convert_to_wav(m4a_file_path, wav_file_path):\n",
    "    sound = AudioSegment.from_file(m4a_file_path, format='m4a')\n",
    "    file_handle = sound.export(wav_file_path, format='wav')\n",
    "    return file_handle\n",
    "\n",
    "def extract_features(audio_file_path):\n",
    "    # Read the audio file\n",
    "    [Fs, X] = audioBasicIO.read_audio_file(audio_file_path)\n",
    "    X = audioBasicIO.stereo_to_mono(X)\n",
    "    F, f_names = ShortTermFeatures.feature_extraction(X, Fs, short_window*Fs, short_step*Fs)\n",
    "    F = np.transpose(F)\n",
    "    print(f'shape: {F.shape}')\n",
    "    return F\n",
    "\n",
    "for filename in os.listdir(audio_files_dir):\n",
    "    if filename.lower().endswith('.m4a') and filename in audio_labels_dict:\n",
    "        m4a_file_path = os.path.join(audio_files_dir, filename)\n",
    "        wav_file_path = m4a_file_path.replace('.m4a', '.wav')\n",
    "\n",
    "        convert_to_wav(m4a_file_path, wav_file_path)\n",
    "\n",
    "        features = extract_features(wav_file_path)\n",
    "        \n",
    "        # Save the feature vectors to a file\n",
    "        feature_vector_path = os.path.join(feature_vectors_dir, filename[:-4] + '.npy')\n",
    "\n",
    "        np.save(feature_vector_path, features)\n",
    "        print(f'Processed {filename}')\n",
    "\n",
    "    elif filename.lower().endswith('.mp3') and filename in audio_labels_dict:\n",
    "        mp3_file_path = os.path.join(audio_files_dir, filename)\n",
    "        features = extract_features(mp3_file_path)\n",
    "        feature_vector_path = os.path.join(feature_vectors_dir, filename[:-4] + '.npy')\n",
    "        np.save(feature_vector_path, features)\n",
    "        print(f'Processed {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "  fnpm3 = f'{filename[:-4]}{str(\".mp3\")}'\n",
    "  fnm4a = f'{filename[:-4]}{str(\".m4a\")}'\n",
    "  if fnpm3 in audio_labels_dict:\n",
    "    y.append(audio_labels_dict[fnpm3])\n",
    "  elif fnm4a in audio_labels_dict:\n",
    "    y.append(audio_labels_dict[fnm4a])\n",
    "\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "    count += 1\n",
    "\n",
    "assert(len(y) == count)\n",
    "\n",
    "max_rows = 0\n",
    "# Iterate through all .npy files in the directory\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(feature_vectors_dir, filename)\n",
    "        \n",
    "        # Load the .npy file\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        # Update the maximum number of rows if this file has more rows\n",
    "        if data.shape[0] > max_rows:\n",
    "            max_rows = data.shape[0]\n",
    "\n",
    "embedding_size = data.shape[1]\n",
    "\n",
    "print(f\"The maximum value of n (number of rows) among all .npy files is: {max_rows}\")\n",
    "\n",
    "X = torch.zeros(len(y), max_rows, embedding_size)\n",
    "currIndex = 0\n",
    "for filename in os.listdir(feature_vectors_dir):\n",
    "  file_path = os.path.join(feature_vectors_dir, filename) \n",
    "  e = torch.from_numpy(np.load(file_path))\n",
    "\n",
    "  fnpm3 = f'{filename[:-4]}{str(\".mp3\")}'\n",
    "  fnm4a = f'{filename[:-4]}{str(\".m4a\")}'\n",
    "\n",
    "  if fnpm3 in audio_labels_dict or fnm4a in audio_labels_dict:\n",
    "    num_vectors_to_pad = max_rows - e.shape[0]\n",
    "    zero_padding = torch.zeros(num_vectors_to_pad, e.shape[1])\n",
    "    padded_tensor = torch.cat((e, zero_padding), dim=0)\n",
    "    X[currIndex] = padded_tensor\n",
    "    currIndex += 1\n",
    "\n",
    "X_np = X.numpy()\n",
    "y_np = y.numpy()\n",
    "np.savez(f'X-y-short-term-audio.npz', X=X_np, y=y_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
